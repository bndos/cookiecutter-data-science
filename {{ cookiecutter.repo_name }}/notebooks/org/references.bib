{% raw %}
@misc{schulman_proximal_2017,
	title = {Proximal Policy Optimization Algorithms},
	url = {http://arxiv.org/abs/1707.06347},
	doi = {10.48550/arXiv.1707.06347},
	abstract = {We propose a new family of policy gradient methods for reinforcement learning, which alternate between sampling data through interaction with the environment, and optimizing a "surrogate" objective function using stochastic gradient ascent. Whereas standard policy gradient methods perform one gradient update per data sample, we propose a novel objective function that enables multiple epochs of minibatch updates. The new methods, which we call proximal policy optimization ({PPO}), have some of the benefits of trust region policy optimization ({TRPO}), but they are much simpler to implement, more general, and have better sample complexity (empirically). Our experiments test {PPO} on a collection of benchmark tasks, including simulated robotic locomotion and Atari game playing, and we show that {PPO} outperforms other online policy gradient methods, and overall strikes a favorable balance between sample complexity, simplicity, and wall-time.},
	number = {{arXiv}:1707.06347},
	publisher = {{arXiv}},
	author = {Schulman, John and Wolski, Filip and Dhariwal, Prafulla and Radford, Alec and Klimov, Oleg},
	urldate = {2023-08-17},
	date = {2017-08-28},
	eprinttype = {arxiv},
	eprint = {1707.06347 [cs]},
	keywords = {Computer Science - Machine Learning},
	file = {arXiv Fulltext PDF:/home/bndo/Zotero/storage/BCXNI4T3/Schulman et al. - 2017 - Proximal Policy Optimization Algorithms.pdf:application/pdf;arXiv.org Snapshot:/home/bndo/Zotero/storage/DI6FZB8J/1707.html:text/html},
}

@inproceedings{huang_gloria_2021,
	location = {Montreal, {QC}, Canada},
	title = {{GLoRIA}: A Multimodal Global-Local Representation Learning Framework for Label-efficient Medical Image Recognition},
	isbn = {978-1-66542-812-5},
	url = {https://ieeexplore.ieee.org/document/9710099/},
	doi = {10.1109/ICCV48922.2021.00391},
	shorttitle = {{GLoRIA}},
	abstract = {In recent years, the growing utilization of medical imaging is placing an increasing burden on radiologists. Deep learning provides a promising solution for automatic medical image analysis and clinical decision support. However, large-scale manually labeled datasets required for training deep neural networks are difficult and expensive to obtain for medical images. The purpose of this work is to develop label-efficient multimodal medical imaging representations by leveraging radiology reports. We propose an attentionbased framework for learning global and local representations by contrasting image sub-regions and words in the paired report. In addition, we propose methods to leverage the learned representations for various downstream medical image recognition tasks with limited labels. Our results demonstrate high-performance and label-efficiency for image-text retrieval, classification (finetuning and zerosshot settings), and segmentation on different datasets.},
	eventtitle = {2021 {IEEE}/{CVF} International Conference on Computer Vision ({ICCV})},
	pages = {3922--3931},
	booktitle = {2021 {IEEE}/{CVF} International Conference on Computer Vision ({ICCV})},
	publisher = {{IEEE}},
	author = {Huang, Shih-Cheng and Shen, Liyue and Lungren, Matthew P. and Yeung, Serena},
	urldate = {2023-12-17},
	date = {2021-10},
	langid = {english},
	file = {gloria-paper.pdf:/home/bndo/School/GBM6700/PROJECT/papers/gloria-paper.pdf:application/pdf},
}

@misc{yang_lavt_2022,
	title = {{LAVT}: Language-Aware Vision Transformer for Referring Image Segmentation},
	url = {http://arxiv.org/abs/2112.02244},
	shorttitle = {{LAVT}},
	abstract = {Referring image segmentation is a fundamental visionlanguage task that aims to segment out an object referred to by a natural language expression from an image. One of the key challenges behind this task is leveraging the referring expression for highlighting relevant positions in the image. A paradigm for tackling this problem is to leverage a powerful vision-language (“cross-modal”) decoder to fuse features independently extracted from a vision encoder and a language encoder. Recent methods have made remarkable advancements in this paradigm by exploiting Transformers as cross-modal decoders, concurrent to the Transformer’s overwhelming success in many other vision-language tasks. Adopting a different approach in this work, we show that signiﬁcantly better cross-modal alignments can be achieved through the early fusion of linguistic and visual features in intermediate layers of a vision Transformer encoder network. By conducting cross-modal feature fusion in the visual feature encoding stage, we can leverage the wellproven correlation modeling power of a Transformer encoder for excavating helpful multi-modal context. This way, accurate segmentation results are readily harvested with a light-weight mask predictor. Without bells and whistles, our method surpasses the previous state-of-the-art methods on {RefCOCO}, {RefCOCO}+, and G-Ref by large margins.},
	number = {{arXiv}:2112.02244},
	publisher = {{arXiv}},
	author = {Yang, Zhao and Wang, Jiaqi and Tang, Yansong and Chen, Kai and Zhao, Hengshuang and Torr, Philip H. S.},
	urldate = {2023-12-17},
	date = {2022-04-05},
	langid = {english},
	eprinttype = {arxiv},
	eprint = {2112.02244 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Computer Vision and Pattern Recognition},
	file = {lavt-paper.pdf:/home/bndo/School/GBM6700/PROJECT/papers/lavt-paper.pdf:application/pdf},
}

@misc{li_lvit_2023,
	title = {{LViT}: Language meets Vision Transformer in Medical Image Segmentation},
	url = {http://arxiv.org/abs/2206.14718},
	shorttitle = {{LViT}},
	abstract = {Deep learning has been widely used in medical image segmentation and other aspects. However, the performance of existing medical image segmentation models has been limited by the challenge of obtaining sufficient high-quality labeled data due to the prohibitive data annotation cost. To alleviate this limitation, we propose a new text-augmented medical image segmentation model {LViT} (Language meets Vision Transformer). In our {LViT} model, medical text annotation is incorporated to compensate for the quality deficiency in image data. In addition, the text information can guide to generate pseudo labels of improved quality in the semi-supervised learning. We also propose an Exponential Pseudo label Iteration mechanism ({EPI}) to help the Pixel-Level Attention Module ({PLAM}) preserve local image features in semi-supervised {LViT} setting. In our model, {LV} (Language-Vision) loss is designed to supervise the training of unlabeled images using text information directly. For evaluation, we construct three multimodal medical segmentation datasets (image + text) containing X-rays and {CT} images. Experimental results show that our proposed {LViT} has superior segmentation performance in both fully-supervised and semisupervised setting. The code and datasets are available at https://github.com/{HUANGLIZI}/{LViT}.},
	number = {{arXiv}:2206.14718},
	publisher = {{arXiv}},
	author = {Li, Zihan and Li, Yunxiang and Li, Qingde and Wang, Puyang and Guo, Dazhou and Lu, Le and Jin, Dakai and Zhang, You and Hong, Qingqi},
	urldate = {2023-12-17},
	date = {2023-06-26},
	langid = {english},
	eprinttype = {arxiv},
	eprint = {2206.14718 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {lvit-paper.pdf:/home/bndo/School/GBM6700/PROJECT/papers/lvit-paper.pdf:application/pdf},
}

@article{isensee_nnu-net_2021,
	title = {{nnU}-Net: a self-configuring method for deep learning-based biomedical image segmentation},
	volume = {18},
	issn = {1548-7091, 1548-7105},
	url = {https://www.nature.com/articles/s41592-020-01008-z},
	doi = {10.1038/s41592-020-01008-z},
	shorttitle = {{nnU}-Net},
	pages = {203--211},
	number = {2},
	journaltitle = {Nature Methods},
	shortjournal = {Nat Methods},
	author = {Isensee, Fabian and Jaeger, Paul F. and Kohl, Simon A. A. and Petersen, Jens and Maier-Hein, Klaus H.},
	urldate = {2023-12-17},
	date = {2021-02},
	langid = {english},
	file = {nnunet-paper.pdf:/home/bndo/School/GBM6700/PROJECT/papers/nnunet-paper.pdf:application/pdf},
}

@inproceedings{chen_see-through-text_2019,
	location = {Seoul, Korea (South)},
	title = {See-Through-Text Grouping for Referring Image Segmentation},
	isbn = {978-1-72814-803-8},
	url = {https://ieeexplore.ieee.org/document/9009843/},
	doi = {10.1109/ICCV.2019.00755},
	abstract = {Motivated by the conventional grouping techniques to image segmentation, we develop their {DNN} counterpart to tackle the referring variant. The proposed method is driven by a convolutional-recurrent neural network ({ConvRNN}) that iteratively carries out top-down processing of bottomup segmentation cues. Given a natural language referring expression, our method learns to predict its relevance to each pixel and derives a See-through-Text Embedding Pixelwise ({STEP}) heatmap, which reveals segmentation cues of pixel level via the learned visual-textual co-embedding. The {ConvRNN} performs a top-down approximation by converting the {STEP} heatmap into a reﬁned one, whereas the improvement is expected from training the network with a classiﬁcation loss from the ground truth. With the reﬁned heatmap, we update the textual representation of the referring expression by re-evaluating its attention distribution and then compute a new {STEP} heatmap as the next input to the {ConvRNN}. Boosting by such collaborative learning, the framework can progressively and simultaneously yield the desired referring segmentation and reasonable attention distribution over the referring sentence. Our method is general and does not rely on, say, the outcomes of object detection from other {DNN} models, while achieving state-ofthe-art performance in all of the four datasets in the experiments.},
	eventtitle = {2019 {IEEE}/{CVF} International Conference on Computer Vision ({ICCV})},
	pages = {7453--7462},
	booktitle = {2019 {IEEE}/{CVF} International Conference on Computer Vision ({ICCV})},
	publisher = {{IEEE}},
	author = {Chen, Ding-Jie and Jia, Songhao and Lo, Yi-Chen and Chen, Hwann-Tzong and Liu, Tyng-Luh},
	urldate = {2023-12-17},
	date = {2019-10},
	langid = {english},
	file = {see-through-text.pdf:/home/bndo/School/GBM6700/PROJECT/papers/see-through-text.pdf:application/pdf},
}

@article{kim_vilt_nodate,
	title = {{ViLT}: Vision-and-Language Transformer Without Convolution or Region Supervision},
	author = {Kim, Wonjae and Son, Bokyung and Kim, Ildoo},
	langid = {english},
	file = {vilt-paper.pdf:/home/bndo/School/GBM6700/PROJECT/papers/vilt-paper.pdf:application/pdf},
}
{% endraw %}
